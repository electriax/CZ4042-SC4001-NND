{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\nnd-proj\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.1.0+cu118\n",
      "Torchvision Version:  0.16.0+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import copy\n",
    "import optuna\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DIR = os.getcwd()\n",
    "MAIN_FOLDER = Path(CURRENT_DIR).parent\n",
    "OUTPUT_FOLDER = os.path.join(MAIN_FOLDER, 'aligned_augmented')  \n",
    "FOLD_DATA = os.path.join(MAIN_FOLDER, 'fold_data') \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "NUM_CLASSES = 2\n",
    "INPUT_SIZE = 224\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transforms\n",
    "def get_data_transforms(input_size):\n",
    "    normalize = [0.485, 0.456, 0.406], [0.229, 0.224, 0.225]\n",
    "    resize = transforms.Resize(input_size + 32)\n",
    "\n",
    "    return {\n",
    "        'train': transforms.Compose([resize, transforms.ToTensor(), transforms.Normalize(*normalize)]),\n",
    "        'val': transforms.Compose([resize, transforms.ToTensor(), transforms.Normalize(*normalize)]),\n",
    "    }\n",
    "\n",
    "data_transforms = get_data_transforms(INPUT_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_gender_data(fold_file):\n",
    "    gender_map = {}\n",
    "    with open(fold_file, 'r') as file:\n",
    "        next(file) \n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 5:\n",
    "                continue  \n",
    "            user_id = parts[0]\n",
    "            gender = parts[4]\n",
    "            if gender.lower() not in [\"m\", \"f\"]:\n",
    "                continue  \n",
    "            gender_map[user_id] = 0 if gender.lower() == \"m\" else 1\n",
    "    return gender_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdienceDataset(Dataset):\n",
    "    def __init__(self, fold_file, data_folder, transform=None):\n",
    "        self.gender_map = load_gender_data(fold_file) \n",
    "        self.data_folder = data_folder\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "\n",
    "        # Iterate through the user_id subfolders for user_id and gender\n",
    "        for user_id in self.gender_map:\n",
    "            user_folder = os.path.join(data_folder, user_id)\n",
    "            if os.path.isdir(user_folder):\n",
    "                for img_name in os.listdir(user_folder):\n",
    "                    if img_name.endswith('.jpg'): \n",
    "                        img_path = os.path.join(user_folder, img_name)\n",
    "                        self.image_paths.append(img_path)\n",
    "                        self.labels.append(self.gender_map[user_id]) \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size, fold_file):\n",
    "    train_dataset = AdienceDataset(fold_file, OUTPUT_FOLDER, transform=data_transforms['train'])\n",
    "    val_dataset = AdienceDataset(fold_file, OUTPUT_FOLDER, transform=data_transforms['val'])\n",
    "    \n",
    "    print(f\"Train size: {len(train_dataset)}, Val size: {len(val_dataset)}\")\n",
    "\n",
    "    if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "        return None \n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    \n",
    "    return {'train': train_loader, 'val': val_loader}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "def load_model(num_classes, input_size):\n",
    "    model = models.efficientnet_v2_m(weights=\"EfficientNet_V2_M_Weights.DEFAULT\")\n",
    "    model.classifier[1] = nn.Linear(1280, num_classes)\n",
    "    model.num_classes = num_classes\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "# Progressively freeze layers\n",
    "def freeze_layers(model, block_no=7):\n",
    "    \"\"\"Freeze layers up to the given block number\"\"\"\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    child = list(model.children())\n",
    "    for i in range(block_no, 9):\n",
    "        for param in child[0][i].parameters():\n",
    "            param.requires_grad = True\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, num_epochs=25, patience=5, trial=None):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    epochs_no_improve = 0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'train_acc': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        for phase in ['train', 'val']:  \n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            all_preds = []\n",
    "            all_labels = []\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
    "            acc = accuracy_score(all_labels, all_preds)\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc.item())             \n",
    "\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                epochs_no_improve = 0\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(f'Training complete with Best Val Acc: {best_acc:.4f}')\n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Objective Function for Optuna\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
    "    block_no = trial.suggest_int(\"block_no\", 4, 7)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "\n",
    "    best_val_acc = 0.0\n",
    "    for fold_idx in range(0, 5):\n",
    "        fold_file = os.path.join(FOLD_DATA, f\"fold_{fold_idx}_data.txt\")\n",
    "        print(f\"Loading fold file: {fold_file}\")\n",
    "        dataloaders = get_dataloaders(batch_size, fold_file)\n",
    "        if dataloaders is None:\n",
    "            print(f\"Skipping fold {fold_idx} due to empty dataset.\")\n",
    "            continue\n",
    "\n",
    "\n",
    "        model = load_model(NUM_CLASSES, INPUT_SIZE)\n",
    "        model = freeze_layers(model, block_no)\n",
    "\n",
    "        params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = optim.Adam(params_to_update, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "        model, history = train_model(model, dataloaders, optimizer, num_epochs=10, trial=trial)\n",
    "\n",
    "        best_val_acc = max(best_val_acc, max(history['val_acc']))\n",
    "\n",
    "    return best_val_acc  # Return the best validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-03-28 23:19:35,188] A new study created in memory with name: no-name-77cd3978-998d-4c75-b6ba-8db676f818af\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading fold file: c:\\Users\\Admin\\Documents\\GitHub\\CZ4042-SC4001-NND\\fold_data\\fold_0_data.txt\n",
      "Train size: 22410, Val size: 22410\n",
      "Epoch 1/10\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameter Optimization Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")  \n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "# Print best hyperparameters\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "print(f\"  Val Accuracy: {trial.value:.4f}\")\n",
    "for key, value in trial.params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the model\n",
    "# torch.save(model, 'efficientnet_test.pth')\n",
    "# print(\"Model saved as efficientnet_test.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
