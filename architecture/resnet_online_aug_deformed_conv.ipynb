{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.6.0+cpu\n",
      "Torchvision Version:  0.21.0+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "# import online augmentation libraries\n",
    "from augmentation_libraries.online_augmentation import augment_image_without_seed\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: C:\\Users\\All Saints\\Desktop\\Uni mods\\SC4001\\CZ4042-SC4001-NND\\architecture\n",
      " Main Folder: C:\\Users\\All Saints\\Desktop\\Uni mods\\SC4001\\CZ4042-SC4001-NND\n",
      " Output Folder: C:\\Users\\All Saints\\Desktop\\Uni mods\\SC4001\\CZ4042-SC4001-NND\\aligned\n",
      " Fold Data Folder: C:\\Users\\All Saints\\Desktop\\Uni mods\\SC4001\\CZ4042-SC4001-NND\\fold_data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CURRENT_DIR = os.getcwd()\n",
    "MAIN_FOLDER = Path(CURRENT_DIR).parent\n",
    "OUTPUT_FOLDER = os.path.join(CURRENT_DIR, 'aligned')  \n",
    "FOLD_DATA = os.path.join(CURRENT_DIR, 'fold_data') \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "cuda_avail = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if cuda_avail else \"cpu\")\n",
    "\n",
    "print(\n",
    "    f\"Current Directory: {CURRENT_DIR}\\n\",\n",
    "    f\"Main Folder: {MAIN_FOLDER}\\n\",\n",
    "    f\"Output Folder: {OUTPUT_FOLDER}\\n\",\n",
    "    f\"Fold Data Folder: {FOLD_DATA}\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transforms\n",
    "def get_data_transforms():\n",
    "    normalize = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "\n",
    "    return {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(*normalize)\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(*normalize)\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(*normalize)\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "data_transforms = get_data_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None, augment=False, num_augmentations=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths (list): List of image file paths.\n",
    "            labels (list): List of corresponding labels.\n",
    "            transform (callable, optional): Transformations to apply to the images.\n",
    "            augment (bool): Whether to apply online augmentation.\n",
    "            num_augmentations (int): Number of augmented versions to create per image.\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.num_augmentations = num_augmentations\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the actual number of samples after augmentation\n",
    "        if self.augment:\n",
    "            return len(self.image_paths) * (self.num_augmentations)\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def get_original_len(self):\n",
    "        \"\"\"Return the number of original images (without augmentation)\"\"\"\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate which original image to use\n",
    "        if self.augment:\n",
    "            original_idx = idx // self.num_augmentations\n",
    "        else:\n",
    "            original_idx = idx\n",
    "            \n",
    "        image = Image.open(self.image_paths[original_idx]).convert('RGB')\n",
    "        label = torch.tensor(self.labels[original_idx], dtype=torch.long)\n",
    "\n",
    "        if self.augment:\n",
    "            # When augmenting, first apply augmentation (which already returns a tensor)\n",
    "            augmented_tensor = augment_image_without_seed(image, final_resolution=(224, 224))\n",
    "            \n",
    "            # Skip ToTensor and only apply normalization if needed\n",
    "            if self.transform:\n",
    "                # Extract the normalization from transform and apply it directly\n",
    "                for t in self.transform.transforms:\n",
    "                    if isinstance(t, transforms.Normalize):\n",
    "                        augmented_tensor = t(augmented_tensor)\n",
    "                        \n",
    "            return augmented_tensor, label\n",
    "\n",
    "        # For non-augmented images, apply the full transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_folds_dataset(image_root, fold_dir, fold_files):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for fold_file in fold_files:\n",
    "        print(f\"Reading fold file: {fold_file}\")\n",
    "        with open(os.path.join(fold_dir, fold_file), 'r') as f:\n",
    "            next(f)  \n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                user_id = parts[0]\n",
    "                original_img_name = parts[1]\n",
    "                gender = parts[4].lower()\n",
    "\n",
    "                if gender not in [\"m\", \"f\"]:\n",
    "                    continue\n",
    "                label = 0 if gender == \"m\" else 1\n",
    "\n",
    "                user_folder = os.path.join(image_root, user_id)\n",
    "                if not os.path.isdir(user_folder):\n",
    "                    continue\n",
    "\n",
    "                for file in os.listdir(user_folder):\n",
    "                    if original_img_name in file:\n",
    "                        full_path = os.path.join(user_folder, file)\n",
    "                        if os.path.isfile(full_path):\n",
    "                            image_paths.append(full_path)\n",
    "                            labels.append(label)\n",
    "                        break\n",
    "\n",
    "    return image_paths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size, train_folds, val_fold):\n",
    "    train_image_paths, train_labels = load_folds_dataset(OUTPUT_FOLDER, FOLD_DATA, train_folds)\n",
    "    val_image_paths, val_labels = load_folds_dataset(OUTPUT_FOLDER, FOLD_DATA, [val_fold])\n",
    "\n",
    "    train_dataset = BasicImageDataset(train_image_paths, train_labels, transform=data_transforms['train'], augment=True)\n",
    "    val_dataset = BasicImageDataset(val_image_paths, val_labels, transform=data_transforms['val'], augment=False)\n",
    "\n",
    "    print(f\"Train size: {len(train_dataset)} (from {train_dataset.get_original_len()} original images)\")\n",
    "    print(f\"Val size: {len(val_dataset)}\")\n",
    "\n",
    "    if train_dataset.get_original_len() == 0 or len(val_dataset) == 0:\n",
    "        return None\n",
    "\n",
    "    num_workers = 2 if cuda_avail else 0\n",
    "    pin_memory = True if cuda_avail else False\n",
    "    # Create DataLoader for training and validation datasets\n",
    "    # Use num_workers and pin_memory only if CUDA is available\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    return {'train': train_loader, 'val': val_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResnetGender(nn.Module):\n",
    "#     def __init__(self, layers=18, pretrained=True, drop_rate=0.3):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         if layers == 18:\n",
    "#             base_model = torchvision.models.resnet18(pretrained=pretrained)\n",
    "#             block_expansion = 1\n",
    "\n",
    "#         self.resnet = nn.Sequential(*list(base_model.children())[:-1]) \n",
    "#         self.pool = nn.AdaptiveAvgPool2d((1, 1))  \n",
    "        \n",
    "#         self.extra_layer = nn.Sequential(\n",
    "#             nn.Linear(block_expansion * 512, 256),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.Dropout(drop_rate),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.gender_predictor = nn.Sequential(\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.Dropout(drop_rate),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.resnet(x)\n",
    "#         x = self.pool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.extra_layer(x)\n",
    "#         return self.gender_predictor(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, eps=0.1, reduction='mean'):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        if self.reduction=='sum':\n",
    "            loss = -log_preds.sum()\n",
    "        else:\n",
    "            loss = -log_preds.sum(dim=-1)\n",
    "            if self.reduction=='mean':\n",
    "                loss = loss.mean()\n",
    "        return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target, reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision.ops import DeformConv2d  # Requires torchvision >= 0.11\n",
    "from torchvision import models\n",
    "\n",
    "# Define a single deformable convolution layer that preserves input dimensions.\n",
    "class SingleDeformableLayer(nn.Module):\n",
    "    def __init__(self, in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1):\n",
    "        super(SingleDeformableLayer, self).__init__()\n",
    "        # Offset predictor: produces 2 offsets per kernel element.\n",
    "        self.offset_conv = nn.Conv2d(in_channels, 2 * kernel_size * kernel_size,\n",
    "                                     kernel_size=kernel_size, stride=stride, padding=padding)\n",
    "        self.deform_conv = DeformConv2d(in_channels, out_channels,\n",
    "                                        kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        offset = self.offset_conv(x)\n",
    "        return self.deform_conv(x, offset)\n",
    "\n",
    "def load_model(drop_rate=0.3):\n",
    "    # Load pretrained ResNet18\n",
    "    base_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "    \n",
    "    # Freeze all parameters in the ResNet backbone\n",
    "    for name, param in base_model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # Replace the fully connected (fc) layer with a new classifier.\n",
    "    # These layers will be trainable.\n",
    "    num_ftrs = base_model.fc.in_features\n",
    "    base_model.fc = nn.Sequential(\n",
    "        nn.Linear(num_ftrs, 128),\n",
    "        nn.BatchNorm1d(128),\n",
    "        nn.Dropout(drop_rate),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, 2)\n",
    "    )\n",
    "    # Ensure the fc layers are trainable.\n",
    "    for param in base_model.fc.parameters():\n",
    "        param.requires_grad = True\n",
    "\n",
    "    # Instantiate the deformable convolution layer.\n",
    "    # This layer takes a 3-channel image and outputs 3 channels (so dimensions match).\n",
    "    deform_layer = SingleDeformableLayer(in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1)\n",
    "    \n",
    "    # Create a new model that first applies the deformable layer, then the frozen ResNet18.\n",
    "    # Add a residual skip connection from the input to the base model.\n",
    "    class DeformableConvWithSkipResnet18(nn.Module):\n",
    "        def __init__(self, deform_layer, base_model):\n",
    "            super(DeformableConvWithSkipResnet18, self).__init__()\n",
    "            self.deform_layer = deform_layer\n",
    "            self.base_model = base_model\n",
    "            self.bn = nn.BatchNorm2d(3)  # Normalize the output of the deformable layer\n",
    "            self.relu = nn.ReLU()        # Activation function after normalization\n",
    "\n",
    "        def forward(self, x):\n",
    "            # Apply the deformable layer\n",
    "            deform_out = self.deform_layer(x)\n",
    "            deform_out = self.bn(deform_out)\n",
    "            deform_out = self.relu(deform_out)\n",
    "            \n",
    "            # Add the residual skip connection\n",
    "            residual = x + deform_out\n",
    "            \n",
    "            # Pass the result through the base model\n",
    "            return self.base_model(residual)\n",
    "\n",
    "    # Instantiate the residual model\n",
    "    new_model = DeformableConvWithSkipResnet18(deform_layer, base_model)\n",
    "    \n",
    "    return new_model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, num_epochs=50, patience=10):\n",
    "    criterion = LabelSmoothingCrossEntropy()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.25)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'train_prec': [], 'val_prec': [],\n",
    "        'train_rec': [], 'val_rec': [],\n",
    "        'train_f1': [], 'val_f1': [],\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n",
    "        for phase in ['train', 'val']:\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            running_loss, running_corrects = 0.0, 0\n",
    "            all_preds, all_labels = [], []\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # Start the minibatch timer\n",
    "                minibatch_start_time = time.time()\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                # End the minibatch timer\n",
    "                minibatch_time = time.time() - minibatch_start_time\n",
    "                print(f\"Minibatch time: {minibatch_time:.2f} seconds\")\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "            epoch_prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "            epoch_rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "            epoch_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc)\n",
    "            history[f'{phase}_prec'].append(epoch_prec)\n",
    "            history[f'{phase}_rec'].append(epoch_rec)\n",
    "            history[f'{phase}_f1'].append(epoch_f1)\n",
    "\n",
    "            print(f\"{phase.upper()} — Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f} | \"\n",
    "                  f\"Prec: {epoch_prec:.4f} | Rec: {epoch_rec:.4f} | F1: {epoch_f1:.4f}\")\n",
    "\n",
    "            if phase == 'val':\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "        if phase == 'train':\n",
    "            scheduler.step()\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f} seconds.\")\n",
    "\n",
    "    def plot_training_curves(history):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Validation Accuracy over Epochs\")\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['val_loss'], label='Validation Loss', color='orange')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Validation Loss over Epochs\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    plot_training_curves(history)\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(f\"\\nTraining complete — Best Val Loss: {best_loss:.4f}\")\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Val = fold_0_data.txt, Train = ['fold_1_data.txt', 'fold_2_data.txt', 'fold_3_data.txt', 'fold_4_data.txt']\n",
      "Reading fold file: fold_1_data.txt\n",
      "Reading fold file: fold_2_data.txt\n",
      "Reading fold file: fold_3_data.txt\n",
      "Reading fold file: fold_4_data.txt\n",
      "Reading fold file: fold_0_data.txt\n",
      "Train size: 67485 (from 13497 original images)\n",
      "Val size: 3995\n",
      "\n",
      "Epoch 1/50\n",
      "Minibatch time: 14.43 seconds\n",
      "Minibatch time: 13.02 seconds\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m params_to_update = [p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m model.parameters() \u001b[38;5;28;01mif\u001b[39;00m p.requires_grad]\n\u001b[32m     11\u001b[39m optimizer = optim.Adam(params_to_update, lr=\u001b[32m0.001\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m model, history = \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m best_val_acc = \u001b[38;5;28mmax\u001b[39m(history[\u001b[33m'\u001b[39m\u001b[33mval_acc\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 32\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, dataloaders, optimizer, num_epochs, patience)\u001b[39m\n\u001b[32m     29\u001b[39m optimizer.zero_grad()\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.set_grad_enabled(phase == \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     loss = criterion(outputs, labels)\n\u001b[32m     34\u001b[39m     preds = torch.argmax(outputs, dim=\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mload_model.<locals>.DeformableConvWithSkipResnet18.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# Apply the deformable layer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     deform_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeform_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     deform_out = \u001b[38;5;28mself\u001b[39m.bn(deform_out)\n\u001b[32m     60\u001b[39m     deform_out = \u001b[38;5;28mself\u001b[39m.relu(deform_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36mSingleDeformableLayer.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     17\u001b[39m     offset = \u001b[38;5;28mself\u001b[39m.offset_conv(x)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdeform_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\torchvision\\ops\\deform_conv.py:170\u001b[39m, in \u001b[36mDeformConv2d.forward\u001b[39m\u001b[34m(self, input, offset, mask)\u001b[39m\n\u001b[32m    161\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, offset: Tensor, mask: Optional[Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m) -> Tensor:\n\u001b[32m    162\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    163\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m    164\u001b[39m \u001b[33;03m        input (Tensor[batch_size, in_channels, in_height, in_width]): input tensor\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    168\u001b[39m \u001b[33;03m            masks to be applied for each position in the convolution kernel.\u001b[39;00m\n\u001b[32m    169\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdeform_conv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    172\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    173\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    174\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    175\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\site-packages\\torchvision\\ops\\deform_conv.py:92\u001b[39m, in \u001b[36mdeform_conv2d\u001b[39m\u001b[34m(input, offset, weight, bias, stride, padding, dilation, mask)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_offset_grps == \u001b[32m0\u001b[39m:\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     87\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mthe shape of the offset tensor at dimension 1 is not valid. It should \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     88\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbe a multiple of 2 * weight.size[2] * weight.size[3].\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     89\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mGot offset.shape[1]=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moffset.shape[\u001b[32m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, while 2 * weight.size[2] * weight.size[3]=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[32m2\u001b[39m\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39mweights_h\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39mweights_w\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     90\u001b[39m     )\n\u001b[32m---> \u001b[39m\u001b[32m92\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mops\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtorchvision\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdeform_conv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     96\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstride_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdil_h\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdil_w\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_weight_grps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_offset_grps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\torch\\_ops.py:1123\u001b[39m, in \u001b[36mOpOverloadPacket.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1121\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_torchbind_op_overload \u001b[38;5;129;01mand\u001b[39;00m _must_dispatch_in_python(args, kwargs):\n\u001b[32m   1122\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _call_overload_packet_from_python(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "all_folds = [f\"fold_{i}_data.txt\" for i in range(5)]\n",
    "for fold_idx in range(5):\n",
    "    val_fold = all_folds[fold_idx]\n",
    "    train_folds = [f for i, f in enumerate(all_folds) if i != fold_idx]\n",
    "    print(f\"Fold {fold_idx}: Val = {val_fold}, Train = {train_folds}\")\n",
    "\n",
    "    dataloaders = get_dataloaders(batch_size=32, train_folds=train_folds, val_fold=val_fold)\n",
    "\n",
    "    model = load_model(drop_rate=0.3)\n",
    "    params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(params_to_update, lr=0.001)\n",
    "\n",
    "    model, history = train_model(model, dataloaders, optimizer, num_epochs=50)\n",
    "    best_val_acc = max(history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as efficientnet_test.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model, 'resnet_test.pth')\n",
    "print(\"Model saved as efficientnet_test.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
