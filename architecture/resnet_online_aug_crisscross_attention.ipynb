{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Version:  2.6.0+cpu\n",
      "Torchvision Version:  0.21.0+cpu\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import copy\n",
    "\n",
    "from pathlib import Path\n",
    "from collections import defaultdict, Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import seaborn as sns\n",
    "import torchvision\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "# import online augmentation libraries\n",
    "from augmentation_libraries.online_augmentation import augment_image_without_seed\n",
    "\n",
    "print(\"PyTorch Version: \",torch.__version__)\n",
    "print(\"Torchvision Version: \",torchvision.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "input dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: C:\\Users\\All Saints\\Desktop\\Uni mods\\SC4001\\CZ4042-SC4001-NND\\architecture\n",
      " Main Folder: C:\\Users\\All Saints\\Desktop\\Uni mods\\SC4001\\CZ4042-SC4001-NND\n",
      " Output Folder: C:\\Users\\All Saints\\Desktop\\Uni mods\\SC4001\\CZ4042-SC4001-NND\\aligned\n",
      " Fold Data Folder: C:\\Users\\All Saints\\Desktop\\Uni mods\\SC4001\\CZ4042-SC4001-NND\\fold_data\n",
      "\n"
     ]
    }
   ],
   "source": [
    "CURRENT_DIR = os.getcwd()\n",
    "MAIN_FOLDER = Path(CURRENT_DIR).parent\n",
    "OUTPUT_FOLDER = os.path.join(CURRENT_DIR, 'aligned')  \n",
    "FOLD_DATA = os.path.join(CURRENT_DIR, 'fold_data') \n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "cuda_avail = torch.cuda.is_available()\n",
    "DEVICE = torch.device(\"cuda\" if cuda_avail else \"cpu\")\n",
    "\n",
    "print(\n",
    "    f\"Current Directory: {CURRENT_DIR}\\n\",\n",
    "    f\"Main Folder: {MAIN_FOLDER}\\n\",\n",
    "    f\"Output Folder: {OUTPUT_FOLDER}\\n\",\n",
    "    f\"Fold Data Folder: {FOLD_DATA}\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Transforms\n",
    "def get_data_transforms():\n",
    "    normalize = [0.5, 0.5, 0.5], [0.5, 0.5, 0.5]\n",
    "\n",
    "    return {\n",
    "        'train': transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(*normalize)\n",
    "        ]),\n",
    "        'val': transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(*normalize)\n",
    "        ]),\n",
    "        'test': transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(*normalize)\n",
    "        ]),\n",
    "    }\n",
    "\n",
    "data_transforms = get_data_transforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None, augment=False, num_augmentations=5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            image_paths (list): List of image file paths.\n",
    "            labels (list): List of corresponding labels.\n",
    "            transform (callable, optional): Transformations to apply to the images.\n",
    "            augment (bool): Whether to apply online augmentation.\n",
    "            num_augmentations (int): Number of augmented versions to create per image.\n",
    "        \"\"\"\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "        self.num_augmentations = num_augmentations\n",
    "        \n",
    "    def __len__(self):\n",
    "        # Return the actual number of samples after augmentation\n",
    "        if self.augment:\n",
    "            return len(self.image_paths) * (self.num_augmentations)\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def get_original_len(self):\n",
    "        \"\"\"Return the number of original images (without augmentation)\"\"\"\n",
    "        return len(self.image_paths)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        # Calculate which original image to use\n",
    "        if self.augment:\n",
    "            original_idx = idx // self.num_augmentations\n",
    "        else:\n",
    "            original_idx = idx\n",
    "            \n",
    "        image = Image.open(self.image_paths[original_idx]).convert('RGB')\n",
    "        label = torch.tensor(self.labels[original_idx], dtype=torch.long)\n",
    "\n",
    "        if self.augment:\n",
    "            # When augmenting, first apply augmentation (which already returns a tensor)\n",
    "            augmented_tensor = augment_image_without_seed(image, final_resolution=(224, 224))\n",
    "            \n",
    "            # Skip ToTensor and only apply normalization if needed\n",
    "            if self.transform:\n",
    "                # Extract the normalization from transform and apply it directly\n",
    "                for t in self.transform.transforms:\n",
    "                    if isinstance(t, transforms.Normalize):\n",
    "                        augmented_tensor = t(augmented_tensor)\n",
    "                        \n",
    "            return augmented_tensor, label\n",
    "\n",
    "        # For non-augmented images, apply the full transform\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        else:\n",
    "            image = transforms.ToTensor()(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_folds_dataset(image_root, fold_dir, fold_files):\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "\n",
    "    for fold_file in fold_files:\n",
    "        print(f\"Reading fold file: {fold_file}\")\n",
    "        with open(os.path.join(fold_dir, fold_file), 'r') as f:\n",
    "            next(f)  \n",
    "            for line in f:\n",
    "                parts = line.strip().split('\\t')\n",
    "                if len(parts) < 5:\n",
    "                    continue\n",
    "                user_id = parts[0]\n",
    "                original_img_name = parts[1]\n",
    "                gender = parts[4].lower()\n",
    "\n",
    "                if gender not in [\"m\", \"f\"]:\n",
    "                    continue\n",
    "                label = 0 if gender == \"m\" else 1\n",
    "\n",
    "                user_folder = os.path.join(image_root, user_id)\n",
    "                if not os.path.isdir(user_folder):\n",
    "                    continue\n",
    "\n",
    "                for file in os.listdir(user_folder):\n",
    "                    if original_img_name in file:\n",
    "                        full_path = os.path.join(user_folder, file)\n",
    "                        if os.path.isfile(full_path):\n",
    "                            image_paths.append(full_path)\n",
    "                            labels.append(label)\n",
    "                        break\n",
    "\n",
    "    return image_paths, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(batch_size, train_folds, val_fold):\n",
    "    train_image_paths, train_labels = load_folds_dataset(OUTPUT_FOLDER, FOLD_DATA, train_folds)\n",
    "    val_image_paths, val_labels = load_folds_dataset(OUTPUT_FOLDER, FOLD_DATA, [val_fold])\n",
    "\n",
    "    train_dataset = BasicImageDataset(train_image_paths, train_labels, transform=data_transforms['train'], augment=True)\n",
    "    val_dataset = BasicImageDataset(val_image_paths, val_labels, transform=data_transforms['val'], augment=False)\n",
    "\n",
    "    print(f\"Train size: {len(train_dataset)} (from {train_dataset.get_original_len()} original images)\")\n",
    "    print(f\"Val size: {len(val_dataset)}\")\n",
    "\n",
    "    if train_dataset.get_original_len() == 0 or len(val_dataset) == 0:\n",
    "        return None\n",
    "\n",
    "    num_workers = 2 if cuda_avail else 0\n",
    "    pin_memory = True if cuda_avail else False\n",
    "    # Create DataLoader for training and validation datasets\n",
    "    # Use num_workers and pin_memory only if CUDA is available\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)\n",
    "\n",
    "    return {'train': train_loader, 'val': val_loader}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ResnetGender(nn.Module):\n",
    "#     def __init__(self, layers=18, pretrained=True, drop_rate=0.3):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         if layers == 18:\n",
    "#             base_model = torchvision.models.resnet18(pretrained=pretrained)\n",
    "#             block_expansion = 1\n",
    "\n",
    "#         self.resnet = nn.Sequential(*list(base_model.children())[:-1]) \n",
    "#         self.pool = nn.AdaptiveAvgPool2d((1, 1))  \n",
    "        \n",
    "#         self.extra_layer = nn.Sequential(\n",
    "#             nn.Linear(block_expansion * 512, 256),\n",
    "#             nn.BatchNorm1d(256),\n",
    "#             nn.Dropout(drop_rate),\n",
    "#             nn.ReLU(),\n",
    "#         )\n",
    "#         self.gender_predictor = nn.Sequential(\n",
    "#             nn.Linear(256, 128),\n",
    "#             nn.BatchNorm1d(128),\n",
    "#             nn.Dropout(drop_rate),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(128, 2)\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.resnet(x)\n",
    "#         x = self.pool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.extra_layer(x)\n",
    "#         return self.gender_predictor(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, eps=0.1, reduction='mean'):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.eps = eps\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        c = output.size()[-1]\n",
    "        log_preds = F.log_softmax(output, dim=-1)\n",
    "        if self.reduction=='sum':\n",
    "            loss = -log_preds.sum()\n",
    "        else:\n",
    "            loss = -log_preds.sum(dim=-1)\n",
    "            if self.reduction=='mean':\n",
    "                loss = loss.mean()\n",
    "        return loss*self.eps/c + (1-self.eps) * F.nll_loss(log_preds, target, reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.criss_cross_attention import CrissCrossAttention\n",
    "# Define the ResNet18 model with Criss-Cross Attention\n",
    "\n",
    "class ResNetCCANet(nn.Module):\n",
    "    def __init__(self, drop_rate=0.3):\n",
    "        super(ResNetCCANet, self).__init__()\n",
    "        # Load the pretrained ResNet18\n",
    "        base_model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "        # Use all layers up to (but not including) the average pooling\n",
    "        self.features = nn.Sequential(\n",
    "            base_model.conv1,\n",
    "            base_model.bn1,\n",
    "            base_model.relu,\n",
    "            base_model.maxpool,\n",
    "            base_model.layer1,\n",
    "            base_model.layer2,\n",
    "            base_model.layer3,\n",
    "            base_model.layer4,\n",
    "        )\n",
    "        \n",
    "        # Freeze the parameters of the ResNet18 backbone\n",
    "        for param in self.features.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Insert two cascaded Criss-Cross Attention modules (CCNet style)\n",
    "        self.cc_attn = nn.Sequential(\n",
    "            CrissCrossAttention(in_dim=512),\n",
    "            CrissCrossAttention(in_dim=512),\n",
    "\n",
    "        )\n",
    "        # Use the original average pooling layer from ResNet18\n",
    "        self.avgpool = base_model.avgpool  # AdaptiveAvgPool2d((1, 1))\n",
    "        # Replace the final fully connected (fc) layer with your custom classifier\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(512, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.Dropout(drop_rate),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)   # Extract features from ResNet18 backbone\n",
    "        x = self.cc_attn(x)    # Apply two Criss-Cross Attention modules\n",
    "        x = self.avgpool(x)    # Global average pooling\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)         # Final classifier\n",
    "        return x\n",
    "\n",
    "def load_model(drop_rate=0.3):\n",
    "    model = ResNetCCANet(drop_rate=drop_rate)\n",
    "    return model.to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloaders, optimizer, num_epochs=50, patience=10):\n",
    "    criterion = LabelSmoothingCrossEntropy()\n",
    "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.25)\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float(\"inf\")\n",
    "    epochs_no_improve = 0\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'train_acc': [], 'val_acc': [],\n",
    "        'train_prec': [], 'val_prec': [],\n",
    "        'train_rec': [], 'val_rec': [],\n",
    "        'train_f1': [], 'val_f1': [],\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'\\nEpoch {epoch + 1}/{num_epochs}')\n",
    "        for phase in ['train', 'val']:\n",
    "            model.train() if phase == 'train' else model.eval()\n",
    "            running_loss, running_corrects = 0.0, 0\n",
    "            all_preds, all_labels = [], []\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                # Start the minibatch timer\n",
    "                minibatch_start_time = time.time()\n",
    "                inputs, labels = inputs.to(DEVICE), labels.to(DEVICE)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                    preds = torch.argmax(outputs, dim=1)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                # End the minibatch timer\n",
    "                minibatch_time = time.time() - minibatch_start_time\n",
    "                print(f\"Minibatch time: {minibatch_time:.2f} seconds\")\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "            epoch_acc = accuracy_score(all_labels, all_preds)\n",
    "            epoch_prec = precision_score(all_labels, all_preds, zero_division=0)\n",
    "            epoch_rec = recall_score(all_labels, all_preds, zero_division=0)\n",
    "            epoch_f1 = f1_score(all_labels, all_preds, zero_division=0)\n",
    "\n",
    "            history[f'{phase}_loss'].append(epoch_loss)\n",
    "            history[f'{phase}_acc'].append(epoch_acc)\n",
    "            history[f'{phase}_prec'].append(epoch_prec)\n",
    "            history[f'{phase}_rec'].append(epoch_rec)\n",
    "            history[f'{phase}_f1'].append(epoch_f1)\n",
    "\n",
    "            print(f\"{phase.upper()} — Loss: {epoch_loss:.4f} | Acc: {epoch_acc:.4f} | \"\n",
    "                  f\"Prec: {epoch_prec:.4f} | Rec: {epoch_rec:.4f} | F1: {epoch_f1:.4f}\")\n",
    "\n",
    "            if phase == 'val':\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    epochs_no_improve = 0\n",
    "                else:\n",
    "                    epochs_no_improve += 1\n",
    "        if phase == 'train':\n",
    "            scheduler.step()\n",
    "            \n",
    "        if epochs_no_improve >= patience:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "        \n",
    "        epoch_time = time.time() - start_time\n",
    "        print(f\"Epoch {epoch + 1} completed in {epoch_time:.2f} seconds.\")\n",
    "\n",
    "    def plot_training_curves(history):\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.title(\"Validation Accuracy over Epochs\")\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(history['val_loss'], label='Validation Loss', color='orange')\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.title(\"Validation Loss over Epochs\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    plot_training_curves(history)\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(f\"\\nTraining complete — Best Val Loss: {best_loss:.4f}\")\n",
    "    return model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0: Val = fold_0_data.txt, Train = ['fold_1_data.txt', 'fold_2_data.txt', 'fold_3_data.txt', 'fold_4_data.txt']\n",
      "Reading fold file: fold_1_data.txt\n",
      "Reading fold file: fold_2_data.txt\n",
      "Reading fold file: fold_3_data.txt\n",
      "Reading fold file: fold_4_data.txt\n",
      "Reading fold file: fold_0_data.txt\n",
      "Train size: 67485 (from 13497 original images)\n",
      "Val size: 3995\n",
      "\n",
      "Epoch 1/50\n",
      "Minibatch time: 3.23 seconds\n",
      "Minibatch time: 1.45 seconds\n",
      "Minibatch time: 1.94 seconds\n",
      "Minibatch time: 1.39 seconds\n",
      "Minibatch time: 1.63 seconds\n",
      "Minibatch time: 1.74 seconds\n",
      "Minibatch time: 1.51 seconds\n",
      "Minibatch time: 1.77 seconds\n",
      "Minibatch time: 1.58 seconds\n",
      "Minibatch time: 1.41 seconds\n",
      "Minibatch time: 1.57 seconds\n",
      "Minibatch time: 1.53 seconds\n",
      "Minibatch time: 2.03 seconds\n",
      "Minibatch time: 1.31 seconds\n",
      "Minibatch time: 1.26 seconds\n",
      "Minibatch time: 1.65 seconds\n",
      "Minibatch time: 1.25 seconds\n",
      "Minibatch time: 1.34 seconds\n",
      "Minibatch time: 1.58 seconds\n",
      "Minibatch time: 1.36 seconds\n",
      "Minibatch time: 1.25 seconds\n",
      "Minibatch time: 1.54 seconds\n",
      "Minibatch time: 1.55 seconds\n",
      "Minibatch time: 1.49 seconds\n",
      "Minibatch time: 1.53 seconds\n",
      "Minibatch time: 1.51 seconds\n",
      "Minibatch time: 1.60 seconds\n",
      "Minibatch time: 1.36 seconds\n",
      "Minibatch time: 1.55 seconds\n",
      "Minibatch time: 1.77 seconds\n",
      "Minibatch time: 2.22 seconds\n",
      "Minibatch time: 1.93 seconds\n",
      "Minibatch time: 1.63 seconds\n",
      "Minibatch time: 1.68 seconds\n",
      "Minibatch time: 1.71 seconds\n",
      "Minibatch time: 2.57 seconds\n",
      "Minibatch time: 1.92 seconds\n",
      "Minibatch time: 2.50 seconds\n",
      "Minibatch time: 2.53 seconds\n",
      "Minibatch time: 2.49 seconds\n",
      "Minibatch time: 2.28 seconds\n",
      "Minibatch time: 2.50 seconds\n",
      "Minibatch time: 2.54 seconds\n",
      "Minibatch time: 2.47 seconds\n",
      "Minibatch time: 2.51 seconds\n",
      "Minibatch time: 2.51 seconds\n",
      "Minibatch time: 2.50 seconds\n",
      "Minibatch time: 2.56 seconds\n",
      "Minibatch time: 2.44 seconds\n",
      "Minibatch time: 2.31 seconds\n",
      "Minibatch time: 2.03 seconds\n",
      "Minibatch time: 1.74 seconds\n",
      "Minibatch time: 2.03 seconds\n",
      "Minibatch time: 1.79 seconds\n",
      "Minibatch time: 2.16 seconds\n",
      "Minibatch time: 1.76 seconds\n",
      "Minibatch time: 2.30 seconds\n",
      "Minibatch time: 2.46 seconds\n",
      "Minibatch time: 2.31 seconds\n",
      "Minibatch time: 2.48 seconds\n",
      "Minibatch time: 2.07 seconds\n",
      "Minibatch time: 2.06 seconds\n",
      "Minibatch time: 2.72 seconds\n",
      "Minibatch time: 2.56 seconds\n",
      "Minibatch time: 2.57 seconds\n",
      "Minibatch time: 2.39 seconds\n",
      "Minibatch time: 2.33 seconds\n",
      "Minibatch time: 2.53 seconds\n",
      "Minibatch time: 2.70 seconds\n",
      "Minibatch time: 2.59 seconds\n",
      "Minibatch time: 2.71 seconds\n",
      "Minibatch time: 2.62 seconds\n",
      "Minibatch time: 2.60 seconds\n",
      "Minibatch time: 2.57 seconds\n",
      "Minibatch time: 2.71 seconds\n",
      "Minibatch time: 2.52 seconds\n",
      "Minibatch time: 2.70 seconds\n",
      "Minibatch time: 2.66 seconds\n",
      "Minibatch time: 2.53 seconds\n",
      "Minibatch time: 2.69 seconds\n",
      "Minibatch time: 2.56 seconds\n",
      "Minibatch time: 2.66 seconds\n",
      "Minibatch time: 2.49 seconds\n",
      "Minibatch time: 2.67 seconds\n",
      "Minibatch time: 2.67 seconds\n",
      "Minibatch time: 2.64 seconds\n",
      "Minibatch time: 2.62 seconds\n",
      "Minibatch time: 2.72 seconds\n",
      "Minibatch time: 2.54 seconds\n",
      "Minibatch time: 2.60 seconds\n",
      "Minibatch time: 2.57 seconds\n",
      "Minibatch time: 2.64 seconds\n",
      "Minibatch time: 2.03 seconds\n",
      "Minibatch time: 1.43 seconds\n",
      "Minibatch time: 1.54 seconds\n",
      "Minibatch time: 1.80 seconds\n",
      "Minibatch time: 1.61 seconds\n",
      "Minibatch time: 1.84 seconds\n",
      "Minibatch time: 1.76 seconds\n",
      "Minibatch time: 1.49 seconds\n",
      "Minibatch time: 1.60 seconds\n",
      "Minibatch time: 1.54 seconds\n",
      "Minibatch time: 1.77 seconds\n",
      "Minibatch time: 1.89 seconds\n",
      "Minibatch time: 1.86 seconds\n",
      "Minibatch time: 1.49 seconds\n",
      "Minibatch time: 1.28 seconds\n",
      "Minibatch time: 2.15 seconds\n",
      "Minibatch time: 2.04 seconds\n",
      "Minibatch time: 1.65 seconds\n",
      "Minibatch time: 1.47 seconds\n",
      "Minibatch time: 1.69 seconds\n",
      "Minibatch time: 1.51 seconds\n",
      "Minibatch time: 1.44 seconds\n",
      "Minibatch time: 2.04 seconds\n",
      "Minibatch time: 1.96 seconds\n",
      "Minibatch time: 1.66 seconds\n",
      "Minibatch time: 1.91 seconds\n",
      "Minibatch time: 1.69 seconds\n",
      "Minibatch time: 1.68 seconds\n",
      "Minibatch time: 1.50 seconds\n",
      "Minibatch time: 1.85 seconds\n",
      "Minibatch time: 1.76 seconds\n",
      "Minibatch time: 3.25 seconds\n",
      "Minibatch time: 2.17 seconds\n",
      "Minibatch time: 2.09 seconds\n",
      "Minibatch time: 1.87 seconds\n",
      "Minibatch time: 3.40 seconds\n",
      "Minibatch time: 2.91 seconds\n",
      "Minibatch time: 2.19 seconds\n",
      "Minibatch time: 2.12 seconds\n",
      "Minibatch time: 2.25 seconds\n",
      "Minibatch time: 2.21 seconds\n",
      "Minibatch time: 1.99 seconds\n",
      "Minibatch time: 2.05 seconds\n",
      "Minibatch time: 1.99 seconds\n",
      "Minibatch time: 1.86 seconds\n",
      "Minibatch time: 1.62 seconds\n",
      "Minibatch time: 1.73 seconds\n",
      "Minibatch time: 1.87 seconds\n",
      "Minibatch time: 1.75 seconds\n",
      "Minibatch time: 2.22 seconds\n",
      "Minibatch time: 1.66 seconds\n",
      "Minibatch time: 2.17 seconds\n",
      "Minibatch time: 2.64 seconds\n",
      "Minibatch time: 3.53 seconds\n",
      "Minibatch time: 1.94 seconds\n",
      "Minibatch time: 2.47 seconds\n",
      "Minibatch time: 1.95 seconds\n",
      "Minibatch time: 2.01 seconds\n",
      "Minibatch time: 1.63 seconds\n",
      "Minibatch time: 2.48 seconds\n",
      "Minibatch time: 1.63 seconds\n",
      "Minibatch time: 1.63 seconds\n",
      "Minibatch time: 2.42 seconds\n",
      "Minibatch time: 4.30 seconds\n",
      "Minibatch time: 2.48 seconds\n",
      "Minibatch time: 2.84 seconds\n",
      "Minibatch time: 2.16 seconds\n",
      "Minibatch time: 1.89 seconds\n",
      "Minibatch time: 2.01 seconds\n",
      "Minibatch time: 2.71 seconds\n",
      "Minibatch time: 1.88 seconds\n",
      "Minibatch time: 2.70 seconds\n",
      "Minibatch time: 2.07 seconds\n",
      "Minibatch time: 2.00 seconds\n",
      "Minibatch time: 2.93 seconds\n",
      "Minibatch time: 2.95 seconds\n",
      "Minibatch time: 3.31 seconds\n",
      "Minibatch time: 2.67 seconds\n",
      "Minibatch time: 2.40 seconds\n",
      "Minibatch time: 2.00 seconds\n",
      "Minibatch time: 1.88 seconds\n",
      "Minibatch time: 2.10 seconds\n",
      "Minibatch time: 2.28 seconds\n",
      "Minibatch time: 3.68 seconds\n",
      "Minibatch time: 2.70 seconds\n",
      "Minibatch time: 2.24 seconds\n",
      "Minibatch time: 1.96 seconds\n",
      "Minibatch time: 2.34 seconds\n",
      "Minibatch time: 1.95 seconds\n",
      "Minibatch time: 2.77 seconds\n",
      "Minibatch time: 1.74 seconds\n",
      "Minibatch time: 2.54 seconds\n",
      "Minibatch time: 1.88 seconds\n",
      "Minibatch time: 1.93 seconds\n",
      "Minibatch time: 1.68 seconds\n",
      "Minibatch time: 1.85 seconds\n",
      "Minibatch time: 1.76 seconds\n",
      "Minibatch time: 1.73 seconds\n",
      "Minibatch time: 2.00 seconds\n",
      "Minibatch time: 1.50 seconds\n",
      "Minibatch time: 2.01 seconds\n",
      "Minibatch time: 1.52 seconds\n",
      "Minibatch time: 1.68 seconds\n",
      "Minibatch time: 1.75 seconds\n",
      "Minibatch time: 1.66 seconds\n",
      "Minibatch time: 2.13 seconds\n",
      "Minibatch time: 1.55 seconds\n",
      "Minibatch time: 3.12 seconds\n",
      "Minibatch time: 2.25 seconds\n",
      "Minibatch time: 2.10 seconds\n",
      "Minibatch time: 2.18 seconds\n",
      "Minibatch time: 2.40 seconds\n",
      "Minibatch time: 2.10 seconds\n",
      "Minibatch time: 2.36 seconds\n",
      "Minibatch time: 2.26 seconds\n",
      "Minibatch time: 2.46 seconds\n",
      "Minibatch time: 1.99 seconds\n",
      "Minibatch time: 1.82 seconds\n",
      "Minibatch time: 2.56 seconds\n",
      "Minibatch time: 2.32 seconds\n",
      "Minibatch time: 1.86 seconds\n",
      "Minibatch time: 2.11 seconds\n",
      "Minibatch time: 2.52 seconds\n",
      "Minibatch time: 2.31 seconds\n",
      "Minibatch time: 2.30 seconds\n",
      "Minibatch time: 2.26 seconds\n",
      "Minibatch time: 2.48 seconds\n",
      "Minibatch time: 2.28 seconds\n",
      "Minibatch time: 2.38 seconds\n",
      "Minibatch time: 2.02 seconds\n",
      "Minibatch time: 2.24 seconds\n",
      "Minibatch time: 2.03 seconds\n",
      "Minibatch time: 2.17 seconds\n",
      "Minibatch time: 2.17 seconds\n",
      "Minibatch time: 2.44 seconds\n",
      "Minibatch time: 2.68 seconds\n",
      "Minibatch time: 2.57 seconds\n",
      "Minibatch time: 2.62 seconds\n",
      "Minibatch time: 2.74 seconds\n",
      "Minibatch time: 2.60 seconds\n",
      "Minibatch time: 3.00 seconds\n",
      "Minibatch time: 3.23 seconds\n",
      "Minibatch time: 3.31 seconds\n",
      "Minibatch time: 3.10 seconds\n",
      "Minibatch time: 2.99 seconds\n",
      "Minibatch time: 3.06 seconds\n",
      "Minibatch time: 3.39 seconds\n",
      "Minibatch time: 3.43 seconds\n",
      "Minibatch time: 3.17 seconds\n",
      "Minibatch time: 3.79 seconds\n",
      "Minibatch time: 2.94 seconds\n",
      "Minibatch time: 4.32 seconds\n",
      "Minibatch time: 3.04 seconds\n",
      "Minibatch time: 3.07 seconds\n",
      "Minibatch time: 2.35 seconds\n",
      "Minibatch time: 3.13 seconds\n",
      "Minibatch time: 2.58 seconds\n",
      "Minibatch time: 3.05 seconds\n",
      "Minibatch time: 2.55 seconds\n",
      "Minibatch time: 2.32 seconds\n",
      "Minibatch time: 2.02 seconds\n",
      "Minibatch time: 1.84 seconds\n",
      "Minibatch time: 2.04 seconds\n",
      "Minibatch time: 2.30 seconds\n",
      "Minibatch time: 2.06 seconds\n",
      "Minibatch time: 2.17 seconds\n",
      "Minibatch time: 2.02 seconds\n",
      "Minibatch time: 2.31 seconds\n",
      "Minibatch time: 2.49 seconds\n",
      "Minibatch time: 2.50 seconds\n",
      "Minibatch time: 2.46 seconds\n",
      "Minibatch time: 2.57 seconds\n",
      "Minibatch time: 2.51 seconds\n",
      "Minibatch time: 2.50 seconds\n",
      "Minibatch time: 2.56 seconds\n",
      "Minibatch time: 3.01 seconds\n",
      "Minibatch time: 3.62 seconds\n",
      "Minibatch time: 2.47 seconds\n",
      "Minibatch time: 2.46 seconds\n",
      "Minibatch time: 4.77 seconds\n",
      "Minibatch time: 3.10 seconds\n",
      "Minibatch time: 2.73 seconds\n",
      "Minibatch time: 2.83 seconds\n",
      "Minibatch time: 3.00 seconds\n",
      "Minibatch time: 2.94 seconds\n",
      "Minibatch time: 2.89 seconds\n",
      "Minibatch time: 3.06 seconds\n",
      "Minibatch time: 2.85 seconds\n",
      "Minibatch time: 3.19 seconds\n",
      "Minibatch time: 3.15 seconds\n",
      "Minibatch time: 3.18 seconds\n",
      "Minibatch time: 2.91 seconds\n",
      "Minibatch time: 2.86 seconds\n",
      "Minibatch time: 2.92 seconds\n",
      "Minibatch time: 3.12 seconds\n",
      "Minibatch time: 2.96 seconds\n",
      "Minibatch time: 2.91 seconds\n",
      "Minibatch time: 3.24 seconds\n",
      "Minibatch time: 3.24 seconds\n",
      "Minibatch time: 2.97 seconds\n",
      "Minibatch time: 3.04 seconds\n",
      "Minibatch time: 2.86 seconds\n",
      "Minibatch time: 2.91 seconds\n",
      "Minibatch time: 2.88 seconds\n",
      "Minibatch time: 2.66 seconds\n",
      "Minibatch time: 2.06 seconds\n",
      "Minibatch time: 1.61 seconds\n",
      "Minibatch time: 3.28 seconds\n"
     ]
    }
   ],
   "source": [
    "all_folds = [f\"fold_{i}_data.txt\" for i in range(5)]\n",
    "for fold_idx in range(5):\n",
    "    val_fold = all_folds[fold_idx]\n",
    "    train_folds = [f for i, f in enumerate(all_folds) if i != fold_idx]\n",
    "    print(f\"Fold {fold_idx}: Val = {val_fold}, Train = {train_folds}\")\n",
    "\n",
    "    dataloaders = get_dataloaders(batch_size=32, train_folds=train_folds, val_fold=val_fold)\n",
    "\n",
    "    model = load_model(drop_rate=0.3)\n",
    "    params_to_update = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = optim.Adam(params_to_update, lr=0.001)\n",
    "\n",
    "    model, history = train_model(model, dataloaders, optimizer, num_epochs=50)\n",
    "    best_val_acc = max(history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as efficientnet_test.pth\n"
     ]
    }
   ],
   "source": [
    "# Save the model\n",
    "torch.save(model, 'resnet_cca_model.pth')\n",
    "# Save the training history\n",
    "torch.save(history, 'training_history.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
